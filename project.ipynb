{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/azhuang/.local/lib/python3.10/site-packages/tqdm/auto.py:21: TqdmWarning: IProgress not found. Please update jupyter and ipywidgets. See https://ipywidgets.readthedocs.io/en/stable/user_install.html\n",
      "  from .autonotebook import tqdm as notebook_tqdm\n"
     ]
    }
   ],
   "source": [
    "import pandas as pd\n",
    "import json\n",
    "from collections import Counter\n",
    "import matplotlib.pyplot as plt\n",
    "import numpy as np\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "from torch.utils.data import Dataset, DataLoader\n",
    "from sklearn.metrics import precision_recall_fscore_support\n",
    "from sklearn.model_selection import train_test_split\n",
    "from transformers import BertTokenizer, BertForSequenceClassification\n",
    "from tqdm import tqdm, trange"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "data_df = pd.read_csv('data/booksummaries.txt', sep='\\t', names=['BookID', 'Thing', 'Title', 'Author', 'Publication Date', 'Genre', 'Summary'])\n",
    "data_df = data_df.drop(columns=['Thing', 'Title', 'Author', 'Publication Date'])\n",
    "data_df = data_df.dropna(ignore_index=True)\n",
    "# data_df ['BookID', 'Genre', 'Summary']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1643.0\n",
      "2598.6369441632273\n"
     ]
    }
   ],
   "source": [
    "words = []\n",
    "for s in data_df['Summary']:\n",
    "    words.append(len(s))\n",
    "print(np.median(words))\n",
    "print(sum(words) / len(data_df['Summary']))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "def process_genre_list(x):\n",
    "    genre_json_string = x['Genre']\n",
    "    genre_list = list(json.loads(genre_json_string).values())\n",
    "    genre_list = [g.lower() for g in genre_list]\n",
    "    x['Genre'] = genre_list\n",
    "    return x"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>BookID</th>\n",
       "      <th>Genre</th>\n",
       "      <th>Summary</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>620</td>\n",
       "      <td>[roman à clef, satire, children's literature, ...</td>\n",
       "      <td>Old Major, the old boar on the Manor Farm, ca...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>843</td>\n",
       "      <td>[science fiction, novella, speculative fiction...</td>\n",
       "      <td>Alex, a teenager living in near-future Englan...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>986</td>\n",
       "      <td>[existentialism, fiction, absurdist fiction, n...</td>\n",
       "      <td>The text of The Plague is divided into five p...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>2080</td>\n",
       "      <td>[hard science fiction, science fiction, specul...</td>\n",
       "      <td>The novel posits that space around the Milky ...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>2152</td>\n",
       "      <td>[war novel, roman à clef]</td>\n",
       "      <td>The book tells the story of Paul Bäumer, a Ge...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>12836</th>\n",
       "      <td>36372465</td>\n",
       "      <td>[science fiction]</td>\n",
       "      <td>The story starts with former government agent...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>12837</th>\n",
       "      <td>36534061</td>\n",
       "      <td>[thriller, fiction, suspense]</td>\n",
       "      <td>The series follows the character of Nick Ston...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>12838</th>\n",
       "      <td>37054020</td>\n",
       "      <td>[thriller, fiction]</td>\n",
       "      <td>The reader first meets Rapp while he is doing...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>12839</th>\n",
       "      <td>37122323</td>\n",
       "      <td>[autobiography]</td>\n",
       "      <td>The book follows very rough chronological ord...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>12840</th>\n",
       "      <td>37159503</td>\n",
       "      <td>[epistolary novel, speculative fiction]</td>\n",
       "      <td>Makar Devushkin and Varvara Dobroselova are s...</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>12841 rows × 3 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "         BookID                                              Genre   \n",
       "0           620  [roman à clef, satire, children's literature, ...  \\\n",
       "1           843  [science fiction, novella, speculative fiction...   \n",
       "2           986  [existentialism, fiction, absurdist fiction, n...   \n",
       "3          2080  [hard science fiction, science fiction, specul...   \n",
       "4          2152                          [war novel, roman à clef]   \n",
       "...         ...                                                ...   \n",
       "12836  36372465                                  [science fiction]   \n",
       "12837  36534061                      [thriller, fiction, suspense]   \n",
       "12838  37054020                                [thriller, fiction]   \n",
       "12839  37122323                                    [autobiography]   \n",
       "12840  37159503            [epistolary novel, speculative fiction]   \n",
       "\n",
       "                                                 Summary  \n",
       "0       Old Major, the old boar on the Manor Farm, ca...  \n",
       "1       Alex, a teenager living in near-future Englan...  \n",
       "2       The text of The Plague is divided into five p...  \n",
       "3       The novel posits that space around the Milky ...  \n",
       "4       The book tells the story of Paul Bäumer, a Ge...  \n",
       "...                                                  ...  \n",
       "12836   The story starts with former government agent...  \n",
       "12837   The series follows the character of Nick Ston...  \n",
       "12838   The reader first meets Rapp while he is doing...  \n",
       "12839   The book follows very rough chronological ord...  \n",
       "12840   Makar Devushkin and Varvara Dobroselova are s...  \n",
       "\n",
       "[12841 rows x 3 columns]"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "data_df = data_df.apply(process_genre_list, axis=1)\n",
    "data_df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "image/png": "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",
      "text/plain": [
       "<Figure size 640x480 with 1 Axes>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "genres = data_df['Genre']\n",
    "c = Counter()\n",
    "for genre_list in genres:\n",
    "    c.update(genre_list)\n",
    "most_common = c.most_common(15)\n",
    "items, counts = zip(*most_common)\n",
    "plt.barh(items, counts)\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "227"
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(c)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0.9056927030605093, 11630.0\n"
     ]
    }
   ],
   "source": [
    "genres_to_find = list(items)\n",
    "count = 0\n",
    "for x in data_df['Genre']:\n",
    "    if any(g in x for g in genres_to_find):\n",
    "        count += 1\n",
    "print(f\"{count/len(data_df['Genre'])}, {count/len(data_df['Genre']) * len(data_df['Genre'])}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "genre_counter = {}\n",
    "for genre in genres_to_find:\n",
    "    genre_counter[genre] = c[genre]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "def filter_genres(x):\n",
    "    # filter dataset to only include top 15 most common genres\n",
    "    x_genres = x['Genre']\n",
    "    x_genres = [g for g in x_genres if g in genre_counter]\n",
    "    if len(x_genres) == 0:\n",
    "        x_genres = np.nan\n",
    "    x['Genre'] = x_genres\n",
    "    return x"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>BookID</th>\n",
       "      <th>Genre</th>\n",
       "      <th>Summary</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>620</td>\n",
       "      <td>[children's literature, speculative fiction, f...</td>\n",
       "      <td>Old Major, the old boar on the Manor Farm, ca...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>843</td>\n",
       "      <td>[science fiction, speculative fiction, fiction]</td>\n",
       "      <td>Alex, a teenager living in near-future Englan...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>986</td>\n",
       "      <td>[fiction, novel]</td>\n",
       "      <td>The text of The Plague is divided into five p...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>2080</td>\n",
       "      <td>[science fiction, speculative fiction, fantasy...</td>\n",
       "      <td>The novel posits that space around the Milky ...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>2890</td>\n",
       "      <td>[children's literature, fantasy, speculative f...</td>\n",
       "      <td>Ged is a young boy on Gont, one of the larger...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>11625</th>\n",
       "      <td>36126792</td>\n",
       "      <td>[young adult literature]</td>\n",
       "      <td>A novel about Annie Stonewall, the daughter o...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>11626</th>\n",
       "      <td>36372465</td>\n",
       "      <td>[science fiction]</td>\n",
       "      <td>The story starts with former government agent...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>11627</th>\n",
       "      <td>36534061</td>\n",
       "      <td>[thriller, fiction, suspense]</td>\n",
       "      <td>The series follows the character of Nick Ston...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>11628</th>\n",
       "      <td>37054020</td>\n",
       "      <td>[thriller, fiction]</td>\n",
       "      <td>The reader first meets Rapp while he is doing...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>11629</th>\n",
       "      <td>37159503</td>\n",
       "      <td>[speculative fiction]</td>\n",
       "      <td>Makar Devushkin and Varvara Dobroselova are s...</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>11630 rows × 3 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "         BookID                                              Genre   \n",
       "0           620  [children's literature, speculative fiction, f...  \\\n",
       "1           843    [science fiction, speculative fiction, fiction]   \n",
       "2           986                                   [fiction, novel]   \n",
       "3          2080  [science fiction, speculative fiction, fantasy...   \n",
       "4          2890  [children's literature, fantasy, speculative f...   \n",
       "...         ...                                                ...   \n",
       "11625  36126792                           [young adult literature]   \n",
       "11626  36372465                                  [science fiction]   \n",
       "11627  36534061                      [thriller, fiction, suspense]   \n",
       "11628  37054020                                [thriller, fiction]   \n",
       "11629  37159503                              [speculative fiction]   \n",
       "\n",
       "                                                 Summary  \n",
       "0       Old Major, the old boar on the Manor Farm, ca...  \n",
       "1       Alex, a teenager living in near-future Englan...  \n",
       "2       The text of The Plague is divided into five p...  \n",
       "3       The novel posits that space around the Milky ...  \n",
       "4       Ged is a young boy on Gont, one of the larger...  \n",
       "...                                                  ...  \n",
       "11625   A novel about Annie Stonewall, the daughter o...  \n",
       "11626   The story starts with former government agent...  \n",
       "11627   The series follows the character of Nick Ston...  \n",
       "11628   The reader first meets Rapp while he is doing...  \n",
       "11629   Makar Devushkin and Varvara Dobroselova are s...  \n",
       "\n",
       "[11630 rows x 3 columns]"
      ]
     },
     "execution_count": 11,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "filtered_df = data_df.apply(filter_genres, axis=1).dropna().reset_index(drop=True)\n",
    "filtered_df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "genre_parents = {\n",
    "    'fiction': [],\n",
    "    'speculative fiction': ['fiction'],\n",
    "    'science fiction': ['speculative fiction', 'fiction'],\n",
    "    'fantasy': ['speculative fiction', 'fiction'],\n",
    "    'horror': ['speculative fiction', 'fiction'],\n",
    "    'novel': ['fiction'],\n",
    "    'historical fiction': ['novel', 'fiction'],\n",
    "    'romance novel': ['novel', 'fiction'],\n",
    "    'thriller': ['novel', 'fiction'],\n",
    "    'suspense': ['novel', 'fiction'],\n",
    "    'crime fiction': ['novel', 'fiction'],\n",
    "    'mystery': ['novel', 'fiction'],\n",
    "    'young adult literature': ['novel', 'fiction'],\n",
    "    \"children's literature\": ['novel', 'fiction'],\n",
    "    'historical novel': ['novel', 'fiction']\n",
    "}\n",
    "genre_to_id = {\n",
    "    'fiction': 0,\n",
    "    'speculative fiction': 1,\n",
    "    'science fiction': 2,\n",
    "    'fantasy': 3,\n",
    "    'horror': 4,\n",
    "    'novel': 5,\n",
    "    'historical fiction': 6,\n",
    "    'romance novel': 7,\n",
    "    'thriller': 8,\n",
    "    'suspense': 9,\n",
    "    'crime fiction': 10,\n",
    "    'mystery': 11,\n",
    "    'young adult literature': 12,\n",
    "    \"children's literature\": 13,\n",
    "    'historical novel': 14\n",
    "}\n",
    "id_to_genre = {v: k for k, v in genre_to_id.items()}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "def generate_genre_vector(x):\n",
    "    genre_list = x['Genre']\n",
    "    genre_vector = np.zeros(len(genre_to_id))\n",
    "    for genre in genre_list:\n",
    "        genre_vector[genre_to_id[genre]] = 1\n",
    "        for gp in genre_parents[genre]:  # mark parent genres as well\n",
    "            genre_vector[genre_to_id[gp]] = 1\n",
    "    x['Label'] = genre_vector\n",
    "    return x"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>BookID</th>\n",
       "      <th>Genre</th>\n",
       "      <th>Summary</th>\n",
       "      <th>Label</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>620</td>\n",
       "      <td>[children's literature, speculative fiction, f...</td>\n",
       "      <td>Old Major, the old boar on the Manor Farm, ca...</td>\n",
       "      <td>[1.0, 1.0, 0.0, 0.0, 0.0, 1.0, 0.0, 0.0, 0.0, ...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>843</td>\n",
       "      <td>[science fiction, speculative fiction, fiction]</td>\n",
       "      <td>Alex, a teenager living in near-future Englan...</td>\n",
       "      <td>[1.0, 1.0, 1.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, ...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>986</td>\n",
       "      <td>[fiction, novel]</td>\n",
       "      <td>The text of The Plague is divided into five p...</td>\n",
       "      <td>[1.0, 0.0, 0.0, 0.0, 0.0, 1.0, 0.0, 0.0, 0.0, ...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>2080</td>\n",
       "      <td>[science fiction, speculative fiction, fantasy...</td>\n",
       "      <td>The novel posits that space around the Milky ...</td>\n",
       "      <td>[1.0, 1.0, 1.0, 1.0, 0.0, 0.0, 0.0, 0.0, 0.0, ...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>2890</td>\n",
       "      <td>[children's literature, fantasy, speculative f...</td>\n",
       "      <td>Ged is a young boy on Gont, one of the larger...</td>\n",
       "      <td>[1.0, 1.0, 0.0, 1.0, 0.0, 1.0, 0.0, 0.0, 0.0, ...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>11625</th>\n",
       "      <td>36126792</td>\n",
       "      <td>[young adult literature]</td>\n",
       "      <td>A novel about Annie Stonewall, the daughter o...</td>\n",
       "      <td>[1.0, 0.0, 0.0, 0.0, 0.0, 1.0, 0.0, 0.0, 0.0, ...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>11626</th>\n",
       "      <td>36372465</td>\n",
       "      <td>[science fiction]</td>\n",
       "      <td>The story starts with former government agent...</td>\n",
       "      <td>[1.0, 1.0, 1.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, ...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>11627</th>\n",
       "      <td>36534061</td>\n",
       "      <td>[thriller, fiction, suspense]</td>\n",
       "      <td>The series follows the character of Nick Ston...</td>\n",
       "      <td>[1.0, 0.0, 0.0, 0.0, 0.0, 1.0, 0.0, 0.0, 1.0, ...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>11628</th>\n",
       "      <td>37054020</td>\n",
       "      <td>[thriller, fiction]</td>\n",
       "      <td>The reader first meets Rapp while he is doing...</td>\n",
       "      <td>[1.0, 0.0, 0.0, 0.0, 0.0, 1.0, 0.0, 0.0, 1.0, ...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>11629</th>\n",
       "      <td>37159503</td>\n",
       "      <td>[speculative fiction]</td>\n",
       "      <td>Makar Devushkin and Varvara Dobroselova are s...</td>\n",
       "      <td>[1.0, 1.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, ...</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>11630 rows × 4 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "         BookID                                              Genre   \n",
       "0           620  [children's literature, speculative fiction, f...  \\\n",
       "1           843    [science fiction, speculative fiction, fiction]   \n",
       "2           986                                   [fiction, novel]   \n",
       "3          2080  [science fiction, speculative fiction, fantasy...   \n",
       "4          2890  [children's literature, fantasy, speculative f...   \n",
       "...         ...                                                ...   \n",
       "11625  36126792                           [young adult literature]   \n",
       "11626  36372465                                  [science fiction]   \n",
       "11627  36534061                      [thriller, fiction, suspense]   \n",
       "11628  37054020                                [thriller, fiction]   \n",
       "11629  37159503                              [speculative fiction]   \n",
       "\n",
       "                                                 Summary   \n",
       "0       Old Major, the old boar on the Manor Farm, ca...  \\\n",
       "1       Alex, a teenager living in near-future Englan...   \n",
       "2       The text of The Plague is divided into five p...   \n",
       "3       The novel posits that space around the Milky ...   \n",
       "4       Ged is a young boy on Gont, one of the larger...   \n",
       "...                                                  ...   \n",
       "11625   A novel about Annie Stonewall, the daughter o...   \n",
       "11626   The story starts with former government agent...   \n",
       "11627   The series follows the character of Nick Ston...   \n",
       "11628   The reader first meets Rapp while he is doing...   \n",
       "11629   Makar Devushkin and Varvara Dobroselova are s...   \n",
       "\n",
       "                                                   Label  \n",
       "0      [1.0, 1.0, 0.0, 0.0, 0.0, 1.0, 0.0, 0.0, 0.0, ...  \n",
       "1      [1.0, 1.0, 1.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, ...  \n",
       "2      [1.0, 0.0, 0.0, 0.0, 0.0, 1.0, 0.0, 0.0, 0.0, ...  \n",
       "3      [1.0, 1.0, 1.0, 1.0, 0.0, 0.0, 0.0, 0.0, 0.0, ...  \n",
       "4      [1.0, 1.0, 0.0, 1.0, 0.0, 1.0, 0.0, 0.0, 0.0, ...  \n",
       "...                                                  ...  \n",
       "11625  [1.0, 0.0, 0.0, 0.0, 0.0, 1.0, 0.0, 0.0, 0.0, ...  \n",
       "11626  [1.0, 1.0, 1.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, ...  \n",
       "11627  [1.0, 0.0, 0.0, 0.0, 0.0, 1.0, 0.0, 0.0, 1.0, ...  \n",
       "11628  [1.0, 0.0, 0.0, 0.0, 0.0, 1.0, 0.0, 0.0, 1.0, ...  \n",
       "11629  [1.0, 1.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, ...  \n",
       "\n",
       "[11630 rows x 4 columns]"
      ]
     },
     "execution_count": 14,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "labeled_df = filtered_df.apply(generate_genre_vector, axis=1).reset_index(drop=True)\n",
    "labeled_df"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Create Train Test Split 80/20"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_df, test_df = train_test_split(labeled_df, test_size=0.2, random_state=42)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Important Variables\n",
    "\n",
    "genre_parents: genre --> list of parents\n",
    "\n",
    "genre_to_id: genre --> id (same order as genre_parents)\n",
    "\n",
    "labeled_df: columns = [BookID, Genre, Summary, Label] (Genre is from the original dataset while Label includes parents) \n",
    "\n",
    "train_df: 80% of labeled_df\n",
    "\n",
    "test_df: 20% of labeled_df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Some weights of BertForSequenceClassification were not initialized from the model checkpoint at bert-large-uncased and are newly initialized: ['classifier.bias', 'classifier.weight']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n"
     ]
    }
   ],
   "source": [
    "tokenizer = BertTokenizer.from_pretrained(\"bert-large-uncased\")\n",
    "num_genres = len(genre_to_id)\n",
    "model = BertForSequenceClassification.from_pretrained(\"bert-large-uncased\", num_labels=num_genres)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [],
   "source": [
    "model.classifier = nn.Sequential(\n",
    "    nn.Linear(model.classifier.in_features, num_genres),\n",
    "    nn.Sigmoid()  # sigmoid activation for multi label\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "BertForSequenceClassification(\n",
       "  (bert): BertModel(\n",
       "    (embeddings): BertEmbeddings(\n",
       "      (word_embeddings): Embedding(30522, 1024, padding_idx=0)\n",
       "      (position_embeddings): Embedding(512, 1024)\n",
       "      (token_type_embeddings): Embedding(2, 1024)\n",
       "      (LayerNorm): LayerNorm((1024,), eps=1e-12, elementwise_affine=True)\n",
       "      (dropout): Dropout(p=0.1, inplace=False)\n",
       "    )\n",
       "    (encoder): BertEncoder(\n",
       "      (layer): ModuleList(\n",
       "        (0-23): 24 x BertLayer(\n",
       "          (attention): BertAttention(\n",
       "            (self): BertSelfAttention(\n",
       "              (query): Linear(in_features=1024, out_features=1024, bias=True)\n",
       "              (key): Linear(in_features=1024, out_features=1024, bias=True)\n",
       "              (value): Linear(in_features=1024, out_features=1024, bias=True)\n",
       "              (dropout): Dropout(p=0.1, inplace=False)\n",
       "            )\n",
       "            (output): BertSelfOutput(\n",
       "              (dense): Linear(in_features=1024, out_features=1024, bias=True)\n",
       "              (LayerNorm): LayerNorm((1024,), eps=1e-12, elementwise_affine=True)\n",
       "              (dropout): Dropout(p=0.1, inplace=False)\n",
       "            )\n",
       "          )\n",
       "          (intermediate): BertIntermediate(\n",
       "            (dense): Linear(in_features=1024, out_features=4096, bias=True)\n",
       "            (intermediate_act_fn): GELUActivation()\n",
       "          )\n",
       "          (output): BertOutput(\n",
       "            (dense): Linear(in_features=4096, out_features=1024, bias=True)\n",
       "            (LayerNorm): LayerNorm((1024,), eps=1e-12, elementwise_affine=True)\n",
       "            (dropout): Dropout(p=0.1, inplace=False)\n",
       "          )\n",
       "        )\n",
       "      )\n",
       "    )\n",
       "    (pooler): BertPooler(\n",
       "      (dense): Linear(in_features=1024, out_features=1024, bias=True)\n",
       "      (activation): Tanh()\n",
       "    )\n",
       "  )\n",
       "  (dropout): Dropout(p=0.1, inplace=False)\n",
       "  (classifier): Sequential(\n",
       "    (0): Linear(in_features=1024, out_features=15, bias=True)\n",
       "    (1): Sigmoid()\n",
       "  )\n",
       ")"
      ]
     },
     "execution_count": 18,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [],
   "source": [
    "class SummaryDataset(Dataset):\n",
    "    def __init__(self, df, tokenizer):\n",
    "        self.df = df\n",
    "        self.tokenizer = tokenizer\n",
    "    \n",
    "    def __len__(self):\n",
    "        return len(self.df)\n",
    "    \n",
    "    def __getitem__(self, idx):\n",
    "        row = self.df.iloc[idx]\n",
    "        inputs = tokenizer(row['Summary'], return_tensors='pt', truncation=True)  # will need to collate\n",
    "        input_ids = inputs['input_ids'].squeeze()\n",
    "        genre_label = torch.tensor(row['Label']).float()\n",
    "        return input_ids, genre_label\n",
    "\n",
    "def collate_fn(batch):\n",
    "    input_ids, labels = zip(*batch)\n",
    "    pad_id = tokenizer.pad_token_id\n",
    "    padded_input_ids = nn.utils.rnn.pad_sequence(\n",
    "        sequences=input_ids,\n",
    "        batch_first=True,\n",
    "        padding_value=pad_id\n",
    "    )\n",
    "    attn_mask = padded_input_ids != pad_id  # 1 when not pad and 0 when pad\n",
    "    labels = torch.stack([l for l in labels])\n",
    "    # labels don't need to be padded bc they're all length 15\n",
    "    return padded_input_ids, attn_mask, labels"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tree depth 3\n"
     ]
    }
   ],
   "source": [
    "tree_depth = max([len(parents) for parents in genre_parents.values()]) + 1\n",
    "print(f\"tree depth {tree_depth}\")\n",
    "\n",
    "def find_most_specific_genres(genre_vec):\n",
    "    # returns the names of the most specific genres\n",
    "    max_depth = 0\n",
    "    most_specific = []\n",
    "    for genre_id, _ in enumerate(genre_vec):\n",
    "        if genre_vec[genre_id] == 0: continue\n",
    "        depth = len(genre_parents[id_to_genre[genre_id]]) + 1\n",
    "        if depth == max_depth:\n",
    "            most_specific.append(id_to_genre[genre_id])\n",
    "        elif depth > max_depth:\n",
    "            max_depth = depth\n",
    "            most_specific = []\n",
    "            most_specific.append(id_to_genre[genre_id])\n",
    "    return most_specific\n",
    "\n",
    "def genre_distance(g1, g2, genre_parents):\n",
    "    if g1 == g2: \n",
    "        return 0\n",
    "    elif (g1 in genre_parents[g2]) or (g2 in genre_parents[g1]):\n",
    "        return abs(len(genre_parents[g1])-len(genre_parents[g2]))\n",
    "    else:\n",
    "        # dist = d(g1, root) + d(g2, root) - 2*d(lca, root)\n",
    "        # lca = lowest common ancestor\n",
    "        lca_dist = 0\n",
    "        if (\"speculative fiction\" in genre_parents[g1]) and (\"speculative fiction\" in genre_parents[g2]):\n",
    "            lca_dist = 1\n",
    "        elif (\"novel\" in genre_parents[g1]) and (\"novel\" in genre_parents[g2]):\n",
    "            lca_dist = 1\n",
    "        return len(genre_parents[g1]) + len(genre_parents[g2]) -2*lca_dist"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [],
   "source": [
    "class HierarchicalLoss(nn.Module):\n",
    "    def __init__(self, genre_parents, genre_to_id, id_to_genre, pred_threshold):\n",
    "        super(HierarchicalLoss, self).__init__()\n",
    "        self.genre_parents = genre_parents\n",
    "        self.genre_to_id = genre_to_id\n",
    "        self.id_to_genre = id_to_genre\n",
    "        self.pred_threshold = pred_threshold\n",
    "    \n",
    "    def forward(self, predictions, targets):\n",
    "        # predictions, targets shape: [batch_size, num_genres]\n",
    "        # first compute binary entropy loss per summary in the batch\n",
    "        base_loss = F.binary_cross_entropy_with_logits(predictions, targets, reduction='none')  # shape [batch_size, num_genres]\n",
    "        per_summary_loss = base_loss.sum(dim=1)  # shape [batch_size, ]\n",
    "        \n",
    "        # extra loss is function of distance between prediction and closest most specific genre\n",
    "        # normalized by some function of tree depth\n",
    "        for i in range(base_loss.shape[0]):\n",
    "            most_specific_genres = find_most_specific_genres(targets[i])\n",
    "            # for all logits over the threshold (predictions), calculate extra loss\n",
    "            for j in range(len(predictions[i])):\n",
    "                if predictions[i][j]>=self.pred_threshold:\n",
    "                    extra_loss = min(\n",
    "                        [genre_distance(m, self.id_to_genre[j], self.genre_parents) for m in most_specific_genres]\n",
    "                    )  # both arguments to genre_distance are strs\n",
    "                    per_summary_loss[i] += extra_loss/np.sqrt(tree_depth)\n",
    "\n",
    "        per_summary_loss /= len(self.genre_parents)  # divide by num genres to get mean per summary\n",
    "        return per_summary_loss.mean()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Training"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "using cuda\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "inner loop:   0%|          | 0/73 [00:04<?, ?it/s]\n",
      "Epoch:   0%|          | 0/10 [00:04<?, ?it/s]\n"
     ]
    },
    {
     "ename": "OutOfMemoryError",
     "evalue": "CUDA out of memory. Tried to allocate 256.00 MiB. GPU 0 has a total capacity of 4.00 GiB of which 0 bytes is free. Including non-PyTorch memory, this process has 17179869184.00 GiB memory in use. Of the allocated memory 18.59 GiB is allocated by PyTorch, and 12.31 MiB is reserved by PyTorch but unallocated. If reserved but unallocated memory is large try setting PYTORCH_CUDA_ALLOC_CONF=expandable_segments:True to avoid fragmentation.  See documentation for Memory Management  (https://pytorch.org/docs/stable/notes/cuda.html#environment-variables)",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mOutOfMemoryError\u001b[0m                          Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[28], line 25\u001b[0m\n\u001b[1;32m     23\u001b[0m input_ids, attn_mask, labels \u001b[38;5;241m=\u001b[39m input_ids\u001b[38;5;241m.\u001b[39mto(device), attn_mask\u001b[38;5;241m.\u001b[39mto(device), labels\u001b[38;5;241m.\u001b[39mto(device)\n\u001b[1;32m     24\u001b[0m optimizer\u001b[38;5;241m.\u001b[39mzero_grad()\n\u001b[0;32m---> 25\u001b[0m outputs \u001b[38;5;241m=\u001b[39m \u001b[43mmodel\u001b[49m\u001b[43m(\u001b[49m\u001b[43minput_ids\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mattention_mask\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mattn_mask\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m     26\u001b[0m loss \u001b[38;5;241m=\u001b[39m hierarchical_loss(outputs\u001b[38;5;241m.\u001b[39mlogits, labels)\n\u001b[1;32m     27\u001b[0m loss\u001b[38;5;241m.\u001b[39mbackward()\n",
      "File \u001b[0;32m~/.local/lib/python3.10/site-packages/torch/nn/modules/module.py:1511\u001b[0m, in \u001b[0;36mModule._wrapped_call_impl\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m   1509\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_compiled_call_impl(\u001b[38;5;241m*\u001b[39margs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs)  \u001b[38;5;66;03m# type: ignore[misc]\u001b[39;00m\n\u001b[1;32m   1510\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[0;32m-> 1511\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_call_impl\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[0;32m~/.local/lib/python3.10/site-packages/torch/nn/modules/module.py:1520\u001b[0m, in \u001b[0;36mModule._call_impl\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m   1515\u001b[0m \u001b[38;5;66;03m# If we don't have any hooks, we want to skip the rest of the logic in\u001b[39;00m\n\u001b[1;32m   1516\u001b[0m \u001b[38;5;66;03m# this function, and just call forward.\u001b[39;00m\n\u001b[1;32m   1517\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m (\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_backward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_forward_pre_hooks\n\u001b[1;32m   1518\u001b[0m         \u001b[38;5;129;01mor\u001b[39;00m _global_backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_backward_hooks\n\u001b[1;32m   1519\u001b[0m         \u001b[38;5;129;01mor\u001b[39;00m _global_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_forward_pre_hooks):\n\u001b[0;32m-> 1520\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mforward_call\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m   1522\u001b[0m \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[1;32m   1523\u001b[0m     result \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mNone\u001b[39;00m\n",
      "File \u001b[0;32m~/.local/lib/python3.10/site-packages/transformers/models/bert/modeling_bert.py:1564\u001b[0m, in \u001b[0;36mBertForSequenceClassification.forward\u001b[0;34m(self, input_ids, attention_mask, token_type_ids, position_ids, head_mask, inputs_embeds, labels, output_attentions, output_hidden_states, return_dict)\u001b[0m\n\u001b[1;32m   1556\u001b[0m \u001b[38;5;250m\u001b[39m\u001b[38;5;124mr\u001b[39m\u001b[38;5;124;03m\"\"\"\u001b[39;00m\n\u001b[1;32m   1557\u001b[0m \u001b[38;5;124;03mlabels (`torch.LongTensor` of shape `(batch_size,)`, *optional*):\u001b[39;00m\n\u001b[1;32m   1558\u001b[0m \u001b[38;5;124;03m    Labels for computing the sequence classification/regression loss. Indices should be in `[0, ...,\u001b[39;00m\n\u001b[1;32m   1559\u001b[0m \u001b[38;5;124;03m    config.num_labels - 1]`. If `config.num_labels == 1` a regression loss is computed (Mean-Square loss), If\u001b[39;00m\n\u001b[1;32m   1560\u001b[0m \u001b[38;5;124;03m    `config.num_labels > 1` a classification loss is computed (Cross-Entropy).\u001b[39;00m\n\u001b[1;32m   1561\u001b[0m \u001b[38;5;124;03m\"\"\"\u001b[39;00m\n\u001b[1;32m   1562\u001b[0m return_dict \u001b[38;5;241m=\u001b[39m return_dict \u001b[38;5;28;01mif\u001b[39;00m return_dict \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m \u001b[38;5;28;01melse\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mconfig\u001b[38;5;241m.\u001b[39muse_return_dict\n\u001b[0;32m-> 1564\u001b[0m outputs \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mbert\u001b[49m\u001b[43m(\u001b[49m\n\u001b[1;32m   1565\u001b[0m \u001b[43m    \u001b[49m\u001b[43minput_ids\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   1566\u001b[0m \u001b[43m    \u001b[49m\u001b[43mattention_mask\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mattention_mask\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   1567\u001b[0m \u001b[43m    \u001b[49m\u001b[43mtoken_type_ids\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mtoken_type_ids\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   1568\u001b[0m \u001b[43m    \u001b[49m\u001b[43mposition_ids\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mposition_ids\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   1569\u001b[0m \u001b[43m    \u001b[49m\u001b[43mhead_mask\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mhead_mask\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   1570\u001b[0m \u001b[43m    \u001b[49m\u001b[43minputs_embeds\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43minputs_embeds\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   1571\u001b[0m \u001b[43m    \u001b[49m\u001b[43moutput_attentions\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43moutput_attentions\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   1572\u001b[0m \u001b[43m    \u001b[49m\u001b[43moutput_hidden_states\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43moutput_hidden_states\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   1573\u001b[0m \u001b[43m    \u001b[49m\u001b[43mreturn_dict\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mreturn_dict\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   1574\u001b[0m \u001b[43m\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m   1576\u001b[0m pooled_output \u001b[38;5;241m=\u001b[39m outputs[\u001b[38;5;241m1\u001b[39m]\n\u001b[1;32m   1578\u001b[0m pooled_output \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mdropout(pooled_output)\n",
      "File \u001b[0;32m~/.local/lib/python3.10/site-packages/torch/nn/modules/module.py:1511\u001b[0m, in \u001b[0;36mModule._wrapped_call_impl\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m   1509\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_compiled_call_impl(\u001b[38;5;241m*\u001b[39margs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs)  \u001b[38;5;66;03m# type: ignore[misc]\u001b[39;00m\n\u001b[1;32m   1510\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[0;32m-> 1511\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_call_impl\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[0;32m~/.local/lib/python3.10/site-packages/torch/nn/modules/module.py:1520\u001b[0m, in \u001b[0;36mModule._call_impl\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m   1515\u001b[0m \u001b[38;5;66;03m# If we don't have any hooks, we want to skip the rest of the logic in\u001b[39;00m\n\u001b[1;32m   1516\u001b[0m \u001b[38;5;66;03m# this function, and just call forward.\u001b[39;00m\n\u001b[1;32m   1517\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m (\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_backward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_forward_pre_hooks\n\u001b[1;32m   1518\u001b[0m         \u001b[38;5;129;01mor\u001b[39;00m _global_backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_backward_hooks\n\u001b[1;32m   1519\u001b[0m         \u001b[38;5;129;01mor\u001b[39;00m _global_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_forward_pre_hooks):\n\u001b[0;32m-> 1520\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mforward_call\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m   1522\u001b[0m \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[1;32m   1523\u001b[0m     result \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mNone\u001b[39;00m\n",
      "File \u001b[0;32m~/.local/lib/python3.10/site-packages/transformers/models/bert/modeling_bert.py:1013\u001b[0m, in \u001b[0;36mBertModel.forward\u001b[0;34m(self, input_ids, attention_mask, token_type_ids, position_ids, head_mask, inputs_embeds, encoder_hidden_states, encoder_attention_mask, past_key_values, use_cache, output_attentions, output_hidden_states, return_dict)\u001b[0m\n\u001b[1;32m   1004\u001b[0m head_mask \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mget_head_mask(head_mask, \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mconfig\u001b[38;5;241m.\u001b[39mnum_hidden_layers)\n\u001b[1;32m   1006\u001b[0m embedding_output \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39membeddings(\n\u001b[1;32m   1007\u001b[0m     input_ids\u001b[38;5;241m=\u001b[39minput_ids,\n\u001b[1;32m   1008\u001b[0m     position_ids\u001b[38;5;241m=\u001b[39mposition_ids,\n\u001b[0;32m   (...)\u001b[0m\n\u001b[1;32m   1011\u001b[0m     past_key_values_length\u001b[38;5;241m=\u001b[39mpast_key_values_length,\n\u001b[1;32m   1012\u001b[0m )\n\u001b[0;32m-> 1013\u001b[0m encoder_outputs \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mencoder\u001b[49m\u001b[43m(\u001b[49m\n\u001b[1;32m   1014\u001b[0m \u001b[43m    \u001b[49m\u001b[43membedding_output\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   1015\u001b[0m \u001b[43m    \u001b[49m\u001b[43mattention_mask\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mextended_attention_mask\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   1016\u001b[0m \u001b[43m    \u001b[49m\u001b[43mhead_mask\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mhead_mask\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   1017\u001b[0m \u001b[43m    \u001b[49m\u001b[43mencoder_hidden_states\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mencoder_hidden_states\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   1018\u001b[0m \u001b[43m    \u001b[49m\u001b[43mencoder_attention_mask\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mencoder_extended_attention_mask\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   1019\u001b[0m \u001b[43m    \u001b[49m\u001b[43mpast_key_values\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mpast_key_values\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   1020\u001b[0m \u001b[43m    \u001b[49m\u001b[43muse_cache\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43muse_cache\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   1021\u001b[0m \u001b[43m    \u001b[49m\u001b[43moutput_attentions\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43moutput_attentions\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   1022\u001b[0m \u001b[43m    \u001b[49m\u001b[43moutput_hidden_states\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43moutput_hidden_states\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   1023\u001b[0m \u001b[43m    \u001b[49m\u001b[43mreturn_dict\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mreturn_dict\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   1024\u001b[0m \u001b[43m\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m   1025\u001b[0m sequence_output \u001b[38;5;241m=\u001b[39m encoder_outputs[\u001b[38;5;241m0\u001b[39m]\n\u001b[1;32m   1026\u001b[0m pooled_output \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mpooler(sequence_output) \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mpooler \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m \u001b[38;5;28;01melse\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m\n",
      "File \u001b[0;32m~/.local/lib/python3.10/site-packages/torch/nn/modules/module.py:1511\u001b[0m, in \u001b[0;36mModule._wrapped_call_impl\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m   1509\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_compiled_call_impl(\u001b[38;5;241m*\u001b[39margs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs)  \u001b[38;5;66;03m# type: ignore[misc]\u001b[39;00m\n\u001b[1;32m   1510\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[0;32m-> 1511\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_call_impl\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[0;32m~/.local/lib/python3.10/site-packages/torch/nn/modules/module.py:1520\u001b[0m, in \u001b[0;36mModule._call_impl\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m   1515\u001b[0m \u001b[38;5;66;03m# If we don't have any hooks, we want to skip the rest of the logic in\u001b[39;00m\n\u001b[1;32m   1516\u001b[0m \u001b[38;5;66;03m# this function, and just call forward.\u001b[39;00m\n\u001b[1;32m   1517\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m (\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_backward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_forward_pre_hooks\n\u001b[1;32m   1518\u001b[0m         \u001b[38;5;129;01mor\u001b[39;00m _global_backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_backward_hooks\n\u001b[1;32m   1519\u001b[0m         \u001b[38;5;129;01mor\u001b[39;00m _global_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_forward_pre_hooks):\n\u001b[0;32m-> 1520\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mforward_call\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m   1522\u001b[0m \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[1;32m   1523\u001b[0m     result \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mNone\u001b[39;00m\n",
      "File \u001b[0;32m~/.local/lib/python3.10/site-packages/transformers/models/bert/modeling_bert.py:607\u001b[0m, in \u001b[0;36mBertEncoder.forward\u001b[0;34m(self, hidden_states, attention_mask, head_mask, encoder_hidden_states, encoder_attention_mask, past_key_values, use_cache, output_attentions, output_hidden_states, return_dict)\u001b[0m\n\u001b[1;32m    596\u001b[0m     layer_outputs \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_gradient_checkpointing_func(\n\u001b[1;32m    597\u001b[0m         layer_module\u001b[38;5;241m.\u001b[39m\u001b[38;5;21m__call__\u001b[39m,\n\u001b[1;32m    598\u001b[0m         hidden_states,\n\u001b[0;32m   (...)\u001b[0m\n\u001b[1;32m    604\u001b[0m         output_attentions,\n\u001b[1;32m    605\u001b[0m     )\n\u001b[1;32m    606\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[0;32m--> 607\u001b[0m     layer_outputs \u001b[38;5;241m=\u001b[39m \u001b[43mlayer_module\u001b[49m\u001b[43m(\u001b[49m\n\u001b[1;32m    608\u001b[0m \u001b[43m        \u001b[49m\u001b[43mhidden_states\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    609\u001b[0m \u001b[43m        \u001b[49m\u001b[43mattention_mask\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    610\u001b[0m \u001b[43m        \u001b[49m\u001b[43mlayer_head_mask\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    611\u001b[0m \u001b[43m        \u001b[49m\u001b[43mencoder_hidden_states\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    612\u001b[0m \u001b[43m        \u001b[49m\u001b[43mencoder_attention_mask\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    613\u001b[0m \u001b[43m        \u001b[49m\u001b[43mpast_key_value\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    614\u001b[0m \u001b[43m        \u001b[49m\u001b[43moutput_attentions\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    615\u001b[0m \u001b[43m    \u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    617\u001b[0m hidden_states \u001b[38;5;241m=\u001b[39m layer_outputs[\u001b[38;5;241m0\u001b[39m]\n\u001b[1;32m    618\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m use_cache:\n",
      "File \u001b[0;32m~/.local/lib/python3.10/site-packages/torch/nn/modules/module.py:1511\u001b[0m, in \u001b[0;36mModule._wrapped_call_impl\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m   1509\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_compiled_call_impl(\u001b[38;5;241m*\u001b[39margs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs)  \u001b[38;5;66;03m# type: ignore[misc]\u001b[39;00m\n\u001b[1;32m   1510\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[0;32m-> 1511\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_call_impl\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[0;32m~/.local/lib/python3.10/site-packages/torch/nn/modules/module.py:1520\u001b[0m, in \u001b[0;36mModule._call_impl\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m   1515\u001b[0m \u001b[38;5;66;03m# If we don't have any hooks, we want to skip the rest of the logic in\u001b[39;00m\n\u001b[1;32m   1516\u001b[0m \u001b[38;5;66;03m# this function, and just call forward.\u001b[39;00m\n\u001b[1;32m   1517\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m (\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_backward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_forward_pre_hooks\n\u001b[1;32m   1518\u001b[0m         \u001b[38;5;129;01mor\u001b[39;00m _global_backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_backward_hooks\n\u001b[1;32m   1519\u001b[0m         \u001b[38;5;129;01mor\u001b[39;00m _global_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_forward_pre_hooks):\n\u001b[0;32m-> 1520\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mforward_call\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m   1522\u001b[0m \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[1;32m   1523\u001b[0m     result \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mNone\u001b[39;00m\n",
      "File \u001b[0;32m~/.local/lib/python3.10/site-packages/transformers/models/bert/modeling_bert.py:497\u001b[0m, in \u001b[0;36mBertLayer.forward\u001b[0;34m(self, hidden_states, attention_mask, head_mask, encoder_hidden_states, encoder_attention_mask, past_key_value, output_attentions)\u001b[0m\n\u001b[1;32m    485\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21mforward\u001b[39m(\n\u001b[1;32m    486\u001b[0m     \u001b[38;5;28mself\u001b[39m,\n\u001b[1;32m    487\u001b[0m     hidden_states: torch\u001b[38;5;241m.\u001b[39mTensor,\n\u001b[0;32m   (...)\u001b[0m\n\u001b[1;32m    494\u001b[0m ) \u001b[38;5;241m-\u001b[39m\u001b[38;5;241m>\u001b[39m Tuple[torch\u001b[38;5;241m.\u001b[39mTensor]:\n\u001b[1;32m    495\u001b[0m     \u001b[38;5;66;03m# decoder uni-directional self-attention cached key/values tuple is at positions 1,2\u001b[39;00m\n\u001b[1;32m    496\u001b[0m     self_attn_past_key_value \u001b[38;5;241m=\u001b[39m past_key_value[:\u001b[38;5;241m2\u001b[39m] \u001b[38;5;28;01mif\u001b[39;00m past_key_value \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m \u001b[38;5;28;01melse\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m\n\u001b[0;32m--> 497\u001b[0m     self_attention_outputs \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mattention\u001b[49m\u001b[43m(\u001b[49m\n\u001b[1;32m    498\u001b[0m \u001b[43m        \u001b[49m\u001b[43mhidden_states\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    499\u001b[0m \u001b[43m        \u001b[49m\u001b[43mattention_mask\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    500\u001b[0m \u001b[43m        \u001b[49m\u001b[43mhead_mask\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    501\u001b[0m \u001b[43m        \u001b[49m\u001b[43moutput_attentions\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43moutput_attentions\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    502\u001b[0m \u001b[43m        \u001b[49m\u001b[43mpast_key_value\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mself_attn_past_key_value\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    503\u001b[0m \u001b[43m    \u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    504\u001b[0m     attention_output \u001b[38;5;241m=\u001b[39m self_attention_outputs[\u001b[38;5;241m0\u001b[39m]\n\u001b[1;32m    506\u001b[0m     \u001b[38;5;66;03m# if decoder, the last output is tuple of self-attn cache\u001b[39;00m\n",
      "File \u001b[0;32m~/.local/lib/python3.10/site-packages/torch/nn/modules/module.py:1511\u001b[0m, in \u001b[0;36mModule._wrapped_call_impl\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m   1509\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_compiled_call_impl(\u001b[38;5;241m*\u001b[39margs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs)  \u001b[38;5;66;03m# type: ignore[misc]\u001b[39;00m\n\u001b[1;32m   1510\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[0;32m-> 1511\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_call_impl\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[0;32m~/.local/lib/python3.10/site-packages/torch/nn/modules/module.py:1520\u001b[0m, in \u001b[0;36mModule._call_impl\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m   1515\u001b[0m \u001b[38;5;66;03m# If we don't have any hooks, we want to skip the rest of the logic in\u001b[39;00m\n\u001b[1;32m   1516\u001b[0m \u001b[38;5;66;03m# this function, and just call forward.\u001b[39;00m\n\u001b[1;32m   1517\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m (\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_backward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_forward_pre_hooks\n\u001b[1;32m   1518\u001b[0m         \u001b[38;5;129;01mor\u001b[39;00m _global_backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_backward_hooks\n\u001b[1;32m   1519\u001b[0m         \u001b[38;5;129;01mor\u001b[39;00m _global_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_forward_pre_hooks):\n\u001b[0;32m-> 1520\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mforward_call\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m   1522\u001b[0m \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[1;32m   1523\u001b[0m     result \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mNone\u001b[39;00m\n",
      "File \u001b[0;32m~/.local/lib/python3.10/site-packages/transformers/models/bert/modeling_bert.py:427\u001b[0m, in \u001b[0;36mBertAttention.forward\u001b[0;34m(self, hidden_states, attention_mask, head_mask, encoder_hidden_states, encoder_attention_mask, past_key_value, output_attentions)\u001b[0m\n\u001b[1;32m    417\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21mforward\u001b[39m(\n\u001b[1;32m    418\u001b[0m     \u001b[38;5;28mself\u001b[39m,\n\u001b[1;32m    419\u001b[0m     hidden_states: torch\u001b[38;5;241m.\u001b[39mTensor,\n\u001b[0;32m   (...)\u001b[0m\n\u001b[1;32m    425\u001b[0m     output_attentions: Optional[\u001b[38;5;28mbool\u001b[39m] \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mFalse\u001b[39;00m,\n\u001b[1;32m    426\u001b[0m ) \u001b[38;5;241m-\u001b[39m\u001b[38;5;241m>\u001b[39m Tuple[torch\u001b[38;5;241m.\u001b[39mTensor]:\n\u001b[0;32m--> 427\u001b[0m     self_outputs \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mself\u001b[49m\u001b[43m(\u001b[49m\n\u001b[1;32m    428\u001b[0m \u001b[43m        \u001b[49m\u001b[43mhidden_states\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    429\u001b[0m \u001b[43m        \u001b[49m\u001b[43mattention_mask\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    430\u001b[0m \u001b[43m        \u001b[49m\u001b[43mhead_mask\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    431\u001b[0m \u001b[43m        \u001b[49m\u001b[43mencoder_hidden_states\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    432\u001b[0m \u001b[43m        \u001b[49m\u001b[43mencoder_attention_mask\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    433\u001b[0m \u001b[43m        \u001b[49m\u001b[43mpast_key_value\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    434\u001b[0m \u001b[43m        \u001b[49m\u001b[43moutput_attentions\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    435\u001b[0m \u001b[43m    \u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    436\u001b[0m     attention_output \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39moutput(self_outputs[\u001b[38;5;241m0\u001b[39m], hidden_states)\n\u001b[1;32m    437\u001b[0m     outputs \u001b[38;5;241m=\u001b[39m (attention_output,) \u001b[38;5;241m+\u001b[39m self_outputs[\u001b[38;5;241m1\u001b[39m:]  \u001b[38;5;66;03m# add attentions if we output them\u001b[39;00m\n",
      "File \u001b[0;32m~/.local/lib/python3.10/site-packages/torch/nn/modules/module.py:1511\u001b[0m, in \u001b[0;36mModule._wrapped_call_impl\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m   1509\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_compiled_call_impl(\u001b[38;5;241m*\u001b[39margs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs)  \u001b[38;5;66;03m# type: ignore[misc]\u001b[39;00m\n\u001b[1;32m   1510\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[0;32m-> 1511\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_call_impl\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[0;32m~/.local/lib/python3.10/site-packages/torch/nn/modules/module.py:1520\u001b[0m, in \u001b[0;36mModule._call_impl\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m   1515\u001b[0m \u001b[38;5;66;03m# If we don't have any hooks, we want to skip the rest of the logic in\u001b[39;00m\n\u001b[1;32m   1516\u001b[0m \u001b[38;5;66;03m# this function, and just call forward.\u001b[39;00m\n\u001b[1;32m   1517\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m (\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_backward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_forward_pre_hooks\n\u001b[1;32m   1518\u001b[0m         \u001b[38;5;129;01mor\u001b[39;00m _global_backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_backward_hooks\n\u001b[1;32m   1519\u001b[0m         \u001b[38;5;129;01mor\u001b[39;00m _global_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_forward_pre_hooks):\n\u001b[0;32m-> 1520\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mforward_call\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m   1522\u001b[0m \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[1;32m   1523\u001b[0m     result \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mNone\u001b[39;00m\n",
      "File \u001b[0;32m~/.local/lib/python3.10/site-packages/transformers/models/bert/modeling_bert.py:309\u001b[0m, in \u001b[0;36mBertSelfAttention.forward\u001b[0;34m(self, hidden_states, attention_mask, head_mask, encoder_hidden_states, encoder_attention_mask, past_key_value, output_attentions)\u001b[0m\n\u001b[1;32m    307\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[1;32m    308\u001b[0m     key_layer \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mtranspose_for_scores(\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mkey(hidden_states))\n\u001b[0;32m--> 309\u001b[0m     value_layer \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mtranspose_for_scores(\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mvalue\u001b[49m\u001b[43m(\u001b[49m\u001b[43mhidden_states\u001b[49m\u001b[43m)\u001b[49m)\n\u001b[1;32m    311\u001b[0m query_layer \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mtranspose_for_scores(mixed_query_layer)\n\u001b[1;32m    313\u001b[0m use_cache \u001b[38;5;241m=\u001b[39m past_key_value \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m\n",
      "File \u001b[0;32m~/.local/lib/python3.10/site-packages/torch/nn/modules/module.py:1511\u001b[0m, in \u001b[0;36mModule._wrapped_call_impl\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m   1509\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_compiled_call_impl(\u001b[38;5;241m*\u001b[39margs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs)  \u001b[38;5;66;03m# type: ignore[misc]\u001b[39;00m\n\u001b[1;32m   1510\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[0;32m-> 1511\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_call_impl\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[0;32m~/.local/lib/python3.10/site-packages/torch/nn/modules/module.py:1520\u001b[0m, in \u001b[0;36mModule._call_impl\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m   1515\u001b[0m \u001b[38;5;66;03m# If we don't have any hooks, we want to skip the rest of the logic in\u001b[39;00m\n\u001b[1;32m   1516\u001b[0m \u001b[38;5;66;03m# this function, and just call forward.\u001b[39;00m\n\u001b[1;32m   1517\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m (\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_backward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_forward_pre_hooks\n\u001b[1;32m   1518\u001b[0m         \u001b[38;5;129;01mor\u001b[39;00m _global_backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_backward_hooks\n\u001b[1;32m   1519\u001b[0m         \u001b[38;5;129;01mor\u001b[39;00m _global_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_forward_pre_hooks):\n\u001b[0;32m-> 1520\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mforward_call\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m   1522\u001b[0m \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[1;32m   1523\u001b[0m     result \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mNone\u001b[39;00m\n",
      "File \u001b[0;32m~/.local/lib/python3.10/site-packages/torch/nn/modules/linear.py:116\u001b[0m, in \u001b[0;36mLinear.forward\u001b[0;34m(self, input)\u001b[0m\n\u001b[1;32m    115\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21mforward\u001b[39m(\u001b[38;5;28mself\u001b[39m, \u001b[38;5;28minput\u001b[39m: Tensor) \u001b[38;5;241m-\u001b[39m\u001b[38;5;241m>\u001b[39m Tensor:\n\u001b[0;32m--> 116\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mF\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mlinear\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;28;43minput\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mweight\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mbias\u001b[49m\u001b[43m)\u001b[49m\n",
      "\u001b[0;31mOutOfMemoryError\u001b[0m: CUDA out of memory. Tried to allocate 256.00 MiB. GPU 0 has a total capacity of 4.00 GiB of which 0 bytes is free. Including non-PyTorch memory, this process has 17179869184.00 GiB memory in use. Of the allocated memory 18.59 GiB is allocated by PyTorch, and 12.31 MiB is reserved by PyTorch but unallocated. If reserved but unallocated memory is large try setting PYTORCH_CUDA_ALLOC_CONF=expandable_segments:True to avoid fragmentation.  See documentation for Memory Management  (https://pytorch.org/docs/stable/notes/cuda.html#environment-variables)"
     ]
    }
   ],
   "source": [
    "device = \"cuda\" if torch.cuda.is_available() else \"cpu\"\n",
    "print(f\"using {device}\")\n",
    "\n",
    "lr = 1e-4\n",
    "epochs = 10\n",
    "batch_size = 128\n",
    "\n",
    "train_dataloader = DataLoader(\n",
    "    SummaryDataset(train_df, tokenizer),\n",
    "    batch_size=128,\n",
    "    shuffle=True,\n",
    "    collate_fn=collate_fn\n",
    ")\n",
    "\n",
    "model = model.to(device)\n",
    "\n",
    "optimizer = torch.optim.AdamW(model.parameters(), lr=lr)\n",
    "hierarchical_loss = HierarchicalLoss(genre_parents, genre_to_id, id_to_genre, pred_threshold=0.5)\n",
    "\n",
    "for epoch in trange(epochs, desc=\"Epoch\"):\n",
    "    model.train()\n",
    "    for input_ids, attn_mask, labels in tqdm(train_dataloader, desc=\"inner loop\", position=1):\n",
    "        input_ids, attn_mask, labels = input_ids.to(device), attn_mask.to(device), labels.to(device)\n",
    "        optimizer.zero_grad()\n",
    "        outputs = model(input_ids, attention_mask=attn_mask)\n",
    "        loss = hierarchical_loss(outputs.logits, labels)\n",
    "        loss.backward()\n",
    "        optimizer.step()\n",
    "    print(f\"Epoch {epoch}: loss {loss.item()}\")\n",
    "    break"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Evaluation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1.0 0.3008355104891487 0.4625265962735389\n"
     ]
    }
   ],
   "source": [
    "# Always fiction\n",
    "\n",
    "y_true = labeled_df['Label']\n",
    "y_true = np.stack(y_true, axis=0)\n",
    "y_pred = np.zeros_like(y_true)\n",
    "y_pred[:,0] = 1\n",
    "precision, recall, fscore, _ = precision_recall_fscore_support(y_true, y_pred, average='micro')\n",
    "print(precision, recall, fscore)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0.2216057476007001, 0.5321099873250731, 0.28458142377326034\n"
     ]
    }
   ],
   "source": [
    "# Random\n",
    "precisions, recalls, fscores = [], [], []\n",
    "m, n = y_true.shape\n",
    "for _ in range(50):\n",
    "    y_pred2 = np.zeros((m,n))\n",
    "    k = np.random.randint(0, m*n)\n",
    "    indices = np.random.choice(m*n, k, replace=False)\n",
    "    y_pred2[np.unravel_index(indices, y_pred2.shape)] = 1\n",
    "    precision, recall, fscore, _ = precision_recall_fscore_support(y_true, y_pred2, average='micro')\n",
    "    precisions.append(precision)\n",
    "    recalls.append(recall)\n",
    "    fscores.append(fscore)\n",
    "print(f\"{sum(precisions)/len(precisions)}, {sum(recalls)/len(recalls)}, {sum(fscores)/len(fscores)}\")\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
